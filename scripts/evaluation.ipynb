{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gpt-4o\", \"gpt-4o-mini\", \"llama3.1-8b-instruct-q4_0\"]\n",
    "setting = [\"\", \"_no_name\"]\n",
    "result_path = \"../results/llms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "results = {}\n",
    "for model in models:\n",
    "    for s in setting:\n",
    "        result_file = os.path.join(result_path, f\"{model}{s}_validation.csv\")\n",
    "        results[f\"{model}{s}\"] = pandas.read_csv(result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = {}\n",
    "\n",
    "# calculate accuracy\n",
    "for model in models:\n",
    "    for s in setting:\n",
    "        result = results[f\"{model}{s}\"]\n",
    "        accuracy[f\"{model}{s}\"] = result[\"valid?\"].sum() / len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt-4o': 0.6363636363636364,\n",
       " 'gpt-4o_no_name': 0.6363636363636364,\n",
       " 'gpt-4o-mini': 0.6666666666666666,\n",
       " 'gpt-4o-mini_no_name': 0.6363636363636364,\n",
       " 'llama3.1-8b-instruct-q4_0': 0.45454545454545453,\n",
       " 'llama3.1-8b-instruct-q4_0_no_name': 0.42424242424242425}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
